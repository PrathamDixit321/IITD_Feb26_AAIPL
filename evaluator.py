#!/usr/bin/python3
"""
Evaluation and Scoring System for AMD AI Premier League (AAIPL)

Scoring Rules:
- A-agent score = (questions correctly answered with correct format / N) * 100
- Q-agent score = (questions incorrectly answered by A-agent / N) * 100
- N = number of format-correct questions
- Teams with <50% format-correct questions are disqualified
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class QuestionValidation:
    """Validation result for a single question"""
    is_valid: bool
    question_id: int
    errors: List[str]
    question_data: Optional[Dict] = None


@dataclass
class AnswerValidation:
    """Validation result for a single answer"""
    is_valid: bool
    question_id: int
    is_correct: bool
    predicted_answer: Optional[str]
    correct_answer: Optional[str]
    errors: List[str]


@dataclass
class MatchResult:
    """Result of a match between two teams"""
    team_a_name: str
    team_b_name: str
    
    # Team A metrics (Team A's Q-agent vs Team B's A-agent)
    team_a_questions_generated: int
    team_a_questions_valid: int
    team_a_questions_valid_pct: float
    team_a_questions_answered_incorrectly: int
    team_a_q_score: float
    
    # Team B metrics (Team B's A-agent answering Team A's questions)
    team_b_answers_submitted: int
    team_b_answers_valid: int
    team_b_answers_correct: int
    team_b_a_score: float
    
    # Team B metrics (Team B's Q-agent vs Team A's A-agent)
    team_b_questions_generated: int
    team_b_questions_valid: int
    team_b_questions_valid_pct: float
    team_b_questions_answered_incorrectly: int
    team_b_q_score: float
    
    # Team A metrics (Team A's A-agent answering Team B's questions)
    team_a_answers_submitted: int
    team_a_answers_valid: int
    team_a_answers_correct: int
    team_a_a_score: float
    
    # Overall scores
    team_a_total_score: float
    team_b_total_score: float
    
    # Disqualification flags
    team_a_disqualified: bool
    team_b_disqualified: bool
    disqualification_reason: Optional[str]
    
    # Winner
    winner: Optional[str]
    match_timestamp: str


class QuestionValidator:
    """Validates questions generated by Q-agent"""
    
    @staticmethod
    def validate_question(question_data: Dict | str, question_id: int) -> QuestionValidation:
        """
        Validate a single question for format correctness
        
        Required fields:
        - topic (string)
        - question (string ending with ?)
        - choices (list of 4 strings in format "A) ...", "B) ...", etc.)
        - answer (one of "A", "B", "C", "D")
        - explanation (string, optional but recommended)
        """
        errors = []
        
        # Parse JSON if string
        if isinstance(question_data, str):
            try:
                # Try to extract JSON from markdown code blocks
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', question_data, re.DOTALL)
                if json_match:
                    question_data = json_match.group(1)
                
                # Try to find JSON object
                json_match = re.search(r'\{.*\}', question_data, re.DOTALL)
                if json_match:
                    question_data = json_match.group(0)
                
                question_data = json.loads(question_data)
            except json.JSONDecodeError as e:
                return QuestionValidation(
                    is_valid=False,
                    question_id=question_id,
                    errors=[f"Invalid JSON format: {str(e)}"]
                )
        
        # Check required fields
        required_fields = ["topic", "question", "choices", "answer"]
        for field in required_fields:
            if field not in question_data:
                errors.append(f"Missing required field: {field}")
        
        if errors:
            return QuestionValidation(
                is_valid=False,
                question_id=question_id,
                errors=errors,
                question_data=question_data
            )
        
        # Validate topic
        if not isinstance(question_data["topic"], str) or len(question_data["topic"].strip()) == 0:
            errors.append("Topic must be a non-empty string")
        
        # Validate question
        question_text = question_data["question"]
        if not isinstance(question_text, str) or len(question_text.strip()) == 0:
            errors.append("Question must be a non-empty string")
        elif not question_text.strip().endswith("?"):
            errors.append("Question must end with a question mark (?)")
        
        # Validate choices
        choices = question_data["choices"]
        if not isinstance(choices, list):
            errors.append("Choices must be a list")
        elif len(choices) != 4:
            errors.append(f"Must have exactly 4 choices, found {len(choices)}")
        else:
            # Validate choice format: "A) ...", "B) ...", etc.
            expected_prefixes = ["A)", "B)", "C)", "D)"]
            for i, (choice, expected) in enumerate(zip(choices, expected_prefixes)):
                if not isinstance(choice, str):
                    errors.append(f"Choice {i+1} must be a string")
                elif not choice.strip().startswith(expected):
                    errors.append(f"Choice {i+1} must start with '{expected}'")
        
        # Validate answer
        answer = question_data["answer"]
        if answer not in ["A", "B", "C", "D"]:
            errors.append(f"Answer must be one of A, B, C, D. Found: {answer}")
        
        # Validate explanation (optional but check if present)
        if "explanation" in question_data:
            if not isinstance(question_data["explanation"], str):
                errors.append("Explanation must be a string if provided")
        
        is_valid = len(errors) == 0
        
        return QuestionValidation(
            is_valid=is_valid,
            question_id=question_id,
            errors=errors,
            question_data=question_data if is_valid else None
        )
    
    @staticmethod
    def validate_questions(questions: List[Dict | str]) -> Tuple[List[QuestionValidation], int, float]:
        """
        Validate multiple questions
        
        Returns:
            - List of validation results
            - Number of valid questions
            - Percentage of valid questions
        """
        validations = []
        
        for i, question in enumerate(questions):
            validation = QuestionValidator.validate_question(question, question_id=i)
            validations.append(validation)
        
        valid_count = sum(1 for v in validations if v.is_valid)
        total_count = len(questions)
        valid_pct = (valid_count / total_count * 100) if total_count > 0 else 0.0
        
        return validations, valid_count, valid_pct


class AnswerValidator:
    """Validates answers generated by A-agent"""
    
    @staticmethod
    def validate_answer(
        answer_data: Dict | str,
        correct_answer: str,
        question_id: int
    ) -> AnswerValidation:
        """
        Validate a single answer for format correctness and accuracy
        
        Expected format:
        {
            "answer": "A/B/C/D",
            "confidence": 0.0-1.0 (optional),
            "reasoning": "..." (optional)
        }
        """
        errors = []
        predicted_answer = None
        
        # Parse JSON if string
        if isinstance(answer_data, str):
            try:
                # Try to extract JSON from markdown code blocks
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', answer_data, re.DOTALL)
                if json_match:
                    answer_data = json_match.group(1)
                
                # Try to find JSON object
                json_match = re.search(r'\{.*\}', answer_data, re.DOTALL)
                if json_match:
                    answer_data = json_match.group(0)
                
                answer_data = json.loads(answer_data)
            except json.JSONDecodeError as e:
                return AnswerValidation(
                    is_valid=False,
                    question_id=question_id,
                    is_correct=False,
                    predicted_answer=None,
                    correct_answer=correct_answer,
                    errors=[f"Invalid JSON format: {str(e)}"]
                )
        
        # Check required field: answer
        if "answer" not in answer_data:
            errors.append("Missing required field: answer")
        else:
            predicted_answer = answer_data["answer"]
            if predicted_answer not in ["A", "B", "C", "D"]:
                errors.append(f"Answer must be one of A, B, C, D. Found: {predicted_answer}")
        
        # Validate confidence if present
        if "confidence" in answer_data:
            confidence = answer_data["confidence"]
            if not isinstance(confidence, (int, float)):
                errors.append("Confidence must be a number")
            elif not (0.0 <= confidence <= 1.0):
                errors.append("Confidence must be between 0.0 and 1.0")
        
        is_valid = len(errors) == 0
        is_correct = is_valid and predicted_answer == correct_answer
        
        return AnswerValidation(
            is_valid=is_valid,
            question_id=question_id,
            is_correct=is_correct,
            predicted_answer=predicted_answer,
            correct_answer=correct_answer,
            errors=errors
        )
    
    @staticmethod
    def validate_answers(
        answers: List[Dict | str],
        questions: List[Dict]
    ) -> Tuple[List[AnswerValidation], int, int]:
        """
        Validate multiple answers against questions
        
        Returns:
            - List of validation results
            - Number of valid answers
            - Number of correct answers
        """
        validations = []
        
        for i, (answer, question) in enumerate(zip(answers, questions)):
            correct_answer = question.get("answer", "")
            validation = AnswerValidator.validate_answer(answer, correct_answer, question_id=i)
            validations.append(validation)
        
        valid_count = sum(1 for v in validations if v.is_valid)
        correct_count = sum(1 for v in validations if v.is_correct)
        
        return validations, valid_count, correct_count


class MatchEvaluator:
    """Evaluates a match between two teams"""
    
    def __init__(self, min_valid_pct: float = 50.0):
        """
        Args:
            min_valid_pct: Minimum percentage of valid questions to avoid disqualification
        """
        self.min_valid_pct = min_valid_pct
    
    def evaluate_match(
        self,
        team_a_name: str,
        team_b_name: str,
        team_a_questions: List[Dict | str],
        team_b_questions: List[Dict | str],
        team_a_answers: List[Dict | str],
        team_b_answers: List[Dict | str]
    ) -> MatchResult:
        """
        Evaluate a complete match between two teams
        
        Args:
            team_a_name: Name of team A
            team_b_name: Name of team B
            team_a_questions: Questions generated by team A's Q-agent
            team_b_questions: Questions generated by team B's Q-agent
            team_a_answers: Answers from team A's A-agent (to team B's questions)
            team_b_answers: Answers from team B's A-agent (to team A's questions)
        
        Returns:
            MatchResult with all scores and metrics
        """
        
        # Validate Team A's questions
        team_a_q_validations, team_a_q_valid, team_a_q_valid_pct = \
            QuestionValidator.validate_questions(team_a_questions)
        
        # Validate Team B's questions
        team_b_q_validations, team_b_q_valid, team_b_q_valid_pct = \
            QuestionValidator.validate_questions(team_b_questions)
        
        # Check for disqualification
        team_a_disqualified = team_a_q_valid_pct < self.min_valid_pct
        team_b_disqualified = team_b_q_valid_pct < self.min_valid_pct
        
        disqualification_reason = None
        if team_a_disqualified and team_b_disqualified:
            disqualification_reason = f"Both teams disqualified: {team_a_name} ({team_a_q_valid_pct:.1f}% valid), {team_b_name} ({team_b_q_valid_pct:.1f}% valid)"
        elif team_a_disqualified:
            disqualification_reason = f"{team_a_name} disqualified: {team_a_q_valid_pct:.1f}% valid questions (minimum: {self.min_valid_pct}%)"
        elif team_b_disqualified:
            disqualification_reason = f"{team_b_name} disqualified: {team_b_q_valid_pct:.1f}% valid questions (minimum: {self.min_valid_pct}%)"
        
        # Extract valid questions only
        team_a_valid_questions = [v.question_data for v in team_a_q_validations if v.is_valid]
        team_b_valid_questions = [v.question_data for v in team_b_q_validations if v.is_valid]
        
        # Validate Team B's answers (to Team A's questions)
        team_b_a_validations, team_b_a_valid, team_b_a_correct = \
            AnswerValidator.validate_answers(team_b_answers[:len(team_a_valid_questions)], team_a_valid_questions)
        
        # Validate Team A's answers (to Team B's questions)
        team_a_a_validations, team_a_a_valid, team_a_a_correct = \
            AnswerValidator.validate_answers(team_a_answers[:len(team_b_valid_questions)], team_b_valid_questions)
        
        # Calculate scores
        N_a = team_a_q_valid  # Number of valid questions from Team A
        N_b = team_b_q_valid  # Number of valid questions from Team B
        
        # Team B's A-agent score (answering Team A's questions)
        team_b_a_score = (team_b_a_correct / N_a * 100) if N_a > 0 else 0.0
        
        # Team A's Q-agent score (questions that Team B answered incorrectly)
        team_a_q_incorrect = N_a - team_b_a_correct
        team_a_q_score = (team_a_q_incorrect / N_a * 100) if N_a > 0 else 0.0
        
        # Team A's A-agent score (answering Team B's questions)
        team_a_a_score = (team_a_a_correct / N_b * 100) if N_b > 0 else 0.0
        
        # Team B's Q-agent score (questions that Team A answered incorrectly)
        team_b_q_incorrect = N_b - team_a_a_correct
        team_b_q_score = (team_b_q_incorrect / N_b * 100) if N_b > 0 else 0.0
        
        # Total scores
        team_a_total_score = team_a_q_score + team_a_a_score
        team_b_total_score = team_b_q_score + team_b_a_score
        
        # Determine winner
        winner = None
        if not team_a_disqualified and not team_b_disqualified:
            if team_a_total_score > team_b_total_score:
                winner = team_a_name
            elif team_b_total_score > team_a_total_score:
                winner = team_b_name
            else:
                winner = "TIE"
        elif team_a_disqualified and not team_b_disqualified:
            winner = team_b_name
        elif team_b_disqualified and not team_a_disqualified:
            winner = team_a_name
        
        return MatchResult(
            team_a_name=team_a_name,
            team_b_name=team_b_name,
            team_a_questions_generated=len(team_a_questions),
            team_a_questions_valid=team_a_q_valid,
            team_a_questions_valid_pct=team_a_q_valid_pct,
            team_a_questions_answered_incorrectly=team_a_q_incorrect,
            team_a_q_score=team_a_q_score,
            team_b_answers_submitted=len(team_b_answers),
            team_b_answers_valid=team_b_a_valid,
            team_b_answers_correct=team_b_a_correct,
            team_b_a_score=team_b_a_score,
            team_b_questions_generated=len(team_b_questions),
            team_b_questions_valid=team_b_q_valid,
            team_b_questions_valid_pct=team_b_q_valid_pct,
            team_b_questions_answered_incorrectly=team_b_q_incorrect,
            team_b_q_score=team_b_q_score,
            team_a_answers_submitted=len(team_a_answers),
            team_a_answers_valid=team_a_a_valid,
            team_a_answers_correct=team_a_a_correct,
            team_a_a_score=team_a_a_score,
            team_a_total_score=team_a_total_score,
            team_b_total_score=team_b_total_score,
            team_a_disqualified=team_a_disqualified,
            team_b_disqualified=team_b_disqualified,
            disqualification_reason=disqualification_reason,
            winner=winner,
            match_timestamp=datetime.now().isoformat()
        )
    
    def save_match_result(self, result: MatchResult, output_path: str):
        """Save match result to JSON file"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, indent=2, ensure_ascii=False)
        
        print(f"Match result saved to: {output_path}")
    
    def print_match_summary(self, result: MatchResult):
        """Print a formatted match summary"""
        
        print("\n" + "="*80)
        print(f"MATCH RESULT: {result.team_a_name} vs {result.team_b_name}")
        print("="*80)
        
        if result.disqualification_reason:
            print(f"\n‚ö†Ô∏è  DISQUALIFICATION: {result.disqualification_reason}\n")
        
        print(f"\n{result.team_a_name} Performance:")
        print(f"  Q-Agent:")
        print(f"    - Questions Generated: {result.team_a_questions_generated}")
        print(f"    - Valid Questions: {result.team_a_questions_valid} ({result.team_a_questions_valid_pct:.1f}%)")
        print(f"    - Questions Answered Incorrectly by Opponent: {result.team_a_questions_answered_incorrectly}")
        print(f"    - Q-Agent Score: {result.team_a_q_score:.2f}")
        print(f"  A-Agent:")
        print(f"    - Answers Submitted: {result.team_a_answers_submitted}")
        print(f"    - Valid Answers: {result.team_a_answers_valid}")
        print(f"    - Correct Answers: {result.team_a_answers_correct}")
        print(f"    - A-Agent Score: {result.team_a_a_score:.2f}")
        print(f"  TOTAL SCORE: {result.team_a_total_score:.2f}")
        
        print(f"\n{result.team_b_name} Performance:")
        print(f"  Q-Agent:")
        print(f"    - Questions Generated: {result.team_b_questions_generated}")
        print(f"    - Valid Questions: {result.team_b_questions_valid} ({result.team_b_questions_valid_pct:.1f}%)")
        print(f"    - Questions Answered Incorrectly by Opponent: {result.team_b_questions_answered_incorrectly}")
        print(f"    - Q-Agent Score: {result.team_b_q_score:.2f}")
        print(f"  A-Agent:")
        print(f"    - Answers Submitted: {result.team_b_answers_submitted}")
        print(f"    - Valid Answers: {result.team_b_answers_valid}")
        print(f"    - Correct Answers: {result.team_b_answers_correct}")
        print(f"    - A-Agent Score: {result.team_b_a_score:.2f}")
        print(f"  TOTAL SCORE: {result.team_b_total_score:.2f}")
        
        print("\n" + "="*80)
        if result.winner == "TIE":
            print("ü§ù RESULT: TIE - Benchmark questions needed for tiebreaker")
        elif result.winner:
            print(f"üèÜ WINNER: {result.winner}")
        print("="*80 + "\n")


if __name__ == "__main__":
    # Example usage
    
    # Sample questions from Team A
    team_a_questions = [
        {
            "topic": "Number Series",
            "difficulty": "hard",
            "question": "What is the next number: 2, 6, 12, 20, 30, ?",
            "choices": ["A) 40", "B) 42", "C) 44", "D) 48"],
            "answer": "B",
            "explanation": "Pattern: n(n+1)"
        },
        {
            "topic": "Probability",
            "question": "Probability of sum 7 with two dice?",
            "choices": ["A) 1/6", "B) 1/12", "C) 1/9", "D) 1/8"],
            "answer": "A",
            "explanation": "6 ways out of 36"
        }
    ]
    
    # Sample questions from Team B
    team_b_questions = [
        {
            "topic": "Algorithms",
            "question": "What is QuickSelect average time complexity?",
            "choices": ["A) O(n log n)", "B) O(n¬≤)", "C) O(n)", "D) O(k log n)"],
            "answer": "C",
            "explanation": "Linear average case"
        }
    ]
    
    # Sample answers from Team A (to Team B's questions)
    team_a_answers = [
        {
            "answer": "C",
            "confidence": 0.92,
            "reasoning": "QuickSelect is O(n) average case"
        }
    ]
    
    # Sample answers from Team B (to Team A's questions)
    team_b_answers = [
        {
            "answer": "B",
            "confidence": 0.95,
            "reasoning": "6√ó7=42"
        },
        {
            "answer": "B",  # Wrong answer
            "confidence": 0.60,
            "reasoning": "Guessing"
        }
    ]
    
    # Evaluate match
    evaluator = MatchEvaluator(min_valid_pct=50.0)
    result = evaluator.evaluate_match(
        team_a_name="Team Alpha",
        team_b_name="Team Beta",
        team_a_questions=team_a_questions,
        team_b_questions=team_b_questions,
        team_a_answers=team_a_answers,
        team_b_answers=team_b_answers
    )
    
    # Print summary
    evaluator.print_match_summary(result)
    
    # Save result
    evaluator.save_match_result(result, "./results/match_result.json")
